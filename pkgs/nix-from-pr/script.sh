# default to stdout, overridable through -o|--output
output="/dev/stdout"

while true; do
	case "$1" in
	-o | --output)
		output="$2"
		shift
		shift
		;;
	-*)
		echo "unknown argument: $1"
		exit 1
		;;
	*)
		break
		;;
	esac
done

if [[ "$#" -ne 2 ]]; then
	echo "usage: $0 <owner/repo> <pr number>"
	exit 1
fi

repo="$1"
pr="$2"

# header
cat > "$output" <<EOF
# autogenerated file
# generated by nix-from-pr
{
  fetchpatch2,
  fetchurl,
}: {
  patches = [
EOF

# generate patches where possible
# TODO: do this properly!! pagination! and in a  real programming language, maybe?
curl "https://api.github.com/repos/${repo}/pulls/${pr}/commits?per_page=100" | \
jq -r 'map(.sha) | .[]' | \
while read -r sha; do
	url="https://github.com/${repo}/commit/${sha}.patch?full_index=1"
	for _ in {0..2}; do
		hash="$(nix-prefetch fetchpatch2 --name "$sha.patch" --url "$url" || true)"
		if [[ -z "$hash" ]]; then
			echo "waiting before retry..."
			sleep 10
		else
			break
		fi
	done
	cat >> "$output" <<EOF
    (fetchpatch2 {
      name = "$sha.patch";
      url = "$url";
      hash = "$hash";
    })
EOF
	sleep 2
done

echo -e '  ];\n  files = [' >> "$output"

# generate additional files that fetchpatch filters out
# TODO: implement pagination for more than 100 files in the pr
curl "https://api.github.com/repos/${repo}/pulls/${pr}/files?per_page=100" | \
jq -c 'map(select(has("patch")|not) | {name:.filename,url:.raw_url}) | .[]' | \
while read -r json; do
	name="$(jq -r '.name' <<<"$json")"
	url="$(jq -r '.url' <<<"$json")"
	hash="$(nix-prefetch fetchurl --url "$url")"
	cat >> "$output" <<EOF
    {
      src = fetchurl {
        url = "$url";
        hash = "$hash";
      };
      name = "$name";
    }
EOF
	sleep 2
done

echo -e '  ];\n}' >> "$output"
